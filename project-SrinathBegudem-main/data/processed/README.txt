The "processed" directory contains the main data folder, which houses the fully scraped and cleaned dataset utilized for analysis. The script clean_data.py is configured to process only a sample of this data—specifically two pages—due to the computational intensity and time constraints associated with handling the entire dataset of 226 pages. This approach ensures that users can quickly verify the functionality of the code without the extensive wait times (approximately 5-6 hours) required for full data processing. The complete and thoroughly cleaned dataset, representing a comprehensive scrape of the specified sources, is readily accessible within this main folder, labeled as 'processed' inside the 'data' directory. This setup is designed for efficient evaluation and demonstration purposes, illustrating the scraping and cleaning processes without overburdening the user with prolonged execution times.
